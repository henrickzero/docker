from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import json
import time

# --- Configura√ß√£o ---
# 1. Instancie o modelo Ollama
# Certifique-se de que o Ollama est√° rodando e o modelo 'llama3' est√° baixado.
llm_local = Ollama(model="llama3")

# 2. Defina a persona de Vendas (System Prompt)
# Esta persona instrui o LLM sobre seu papel e o fluxo de vendas.
system_prompt = (
    "Voc√™ √© 'Bot de Vendas Consultivas', um assistente amig√°vel e focado no cliente. "
    "Seu objetivo √© guiar o cliente atrav√©s de um funil de vendas simples, seguindo estes passos:"
    "1. **Boas-vindas:** Se apresente e pergunte o objetivo do cliente."
    "2. **Sondagem (Necessidade):** Fa√ßa 1 ou 2 perguntas para entender a dor e o contexto do cliente."
    "3. **Oferta/Solu√ß√£o:** Sugira um produto/servi√ßo relevante (mencione 'Solu√ß√µes Digitais')."
    "4. **Pr√≥ximo Passo:** Pe√ßa o e-mail ou telefone para agendar uma demonstra√ß√£o."
    "**Mantenha cada resposta curta e focada em avan√ßar o di√°logo.**"
)

# O prompt principal que ser√° usado no fluxo. Ele aceita o hist√≥rico do chat.
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("user", "{input}"),
])

# 3. Crie a Chain (Prompt | LLM)
chain = prompt | llm_local

# ----------------- Simula√ß√£o do Fluxo de Vendas no Chat -----------------

print("===============================================")
print("ü§ñ IN√çCIO DO CHAT DE VENDAS (Modelo: Llama 3)")
print("===============================================")

# Passo 1: In√≠cio (Pergunta do Cliente)
user_input = "Ol√°, estou pensando em modernizar os sistemas da minha empresa."

print(f"\n[CLIENTE]: {user_input}")
time.sleep(1) # Simula o tempo de espera
bot_response = chain.invoke({"input": user_input})
print(f"\n[BOT DE VENDAS]: {bot_response.strip()}")

# Passo 2: Sondagem (Continua√ß√£o)
# O bot deve responder fazendo perguntas sobre a dor (Gra√ßas ao System Prompt)
user_input_2 = "Ah, sim. Atualmente nossa maior dor √© a lentid√£o do nosso sistema de CRM."

print(f"\n[CLIENTE]: {user_input_2}")
time.sleep(1)
# Aqui, o LLM usa o hist√≥rico impl√≠cito (mantido pelo usu√°rio) + o System Prompt para saber como continuar
# Nota: Para um chat persistente real, voc√™ usaria um objeto 'ConversationalChain' que armazena o hist√≥rico!
bot_response_2 = chain.invoke({"input": f"{user_input_2}. O que voc√™ me sugere agora?"})
print(f"\n[BOT DE VENDAS]: {bot_response_2.strip()}")

# Passo 3: Fechamento/Pr√≥ximo Passo (Continua√ß√£o)
user_input_3 = "Parece interessante. Como podemos avan√ßar?"

print(f"\n[CLIENTE]: {user_input_3}")
time.sleep(1)
bot_response_3 = chain.invoke({"input": f"{user_input_3}. O que voc√™ me sugere agora?"})
print(f"\n[BOT DE VENDAS]: {bot_response_3.strip()}")

print("\n===============================================")
print("‚úÖ FIM DA SIMULA√á√ÉO")